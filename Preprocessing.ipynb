{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12488898,"sourceType":"datasetVersion","datasetId":7880985},{"sourceId":12489025,"sourceType":"datasetVersion","datasetId":7881077}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom nltk.corpus import stopwords\nimport os\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ntrain = pd.read_csv('/kaggle/input/subtask1/train.csv')\ndev = pd.read_csv('/kaggle/input/subtask1/validation.csv')\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:03.991297Z","iopub.execute_input":"2025-07-17T14:06:03.991503Z","iopub.status.idle":"2025-07-17T14:06:08.580092Z","shell.execute_reply.started":"2025-07-17T14:06:03.991482Z","shell.execute_reply":"2025-07-17T14:06:08.579330Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"     id                                               text           label\n0  8167                       و لانني احب الاشياء الراقيه   not_applicable\n1  1532  أَثِقُ فِي قُدْرَتِي عَلَى التَّعَامُلِ مَعَ ا...            hope\n2  4710  وروضة بات طل الغيث ينسجها حتى إذا نسجت أضحى يد...  not_applicable\n3  6084  أَشْعُرُ بِقَلْبِي العَنِيدِ وَهُوَ يَحَارُبُ ...  not_applicable\n4  8968  @MoaElshamy بتحسسني اني مرتضى ده كان بيدي واحد...  not_applicable","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8167</td>\n      <td>و لانني احب الاشياء الراقيه</td>\n      <td>not_applicable</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1532</td>\n      <td>أَثِقُ فِي قُدْرَتِي عَلَى التَّعَامُلِ مَعَ ا...</td>\n      <td>hope</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4710</td>\n      <td>وروضة بات طل الغيث ينسجها حتى إذا نسجت أضحى يد...</td>\n      <td>not_applicable</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6084</td>\n      <td>أَشْعُرُ بِقَلْبِي العَنِيدِ وَهُوَ يَحَارُبُ ...</td>\n      <td>not_applicable</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8968</td>\n      <td>@MoaElshamy بتحسسني اني مرتضى ده كان بيدي واحد...</td>\n      <td>not_applicable</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"train.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:13.760879Z","iopub.execute_input":"2025-07-17T14:06:13.761189Z","iopub.status.idle":"2025-07-17T14:06:13.785982Z","shell.execute_reply.started":"2025-07-17T14:06:13.761150Z","shell.execute_reply":"2025-07-17T14:06:13.785282Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"dev.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:16.117483Z","iopub.execute_input":"2025-07-17T14:06:16.117811Z","iopub.status.idle":"2025-07-17T14:06:16.127099Z","shell.execute_reply.started":"2025-07-17T14:06:16.117762Z","shell.execute_reply":"2025-07-17T14:06:16.126336Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:19.753250Z","iopub.execute_input":"2025-07-17T14:06:19.753578Z","iopub.status.idle":"2025-07-17T14:06:19.774098Z","shell.execute_reply.started":"2025-07-17T14:06:19.753552Z","shell.execute_reply":"2025-07-17T14:06:19.773274Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6890 entries, 0 to 6889\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      6890 non-null   int64 \n 1   text    6890 non-null   object\n 2   label   6890 non-null   object\ndtypes: int64(1), object(2)\nmemory usage: 161.6+ KB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"dev.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:22.503567Z","iopub.execute_input":"2025-07-17T14:06:22.503906Z","iopub.status.idle":"2025-07-17T14:06:22.513709Z","shell.execute_reply.started":"2025-07-17T14:06:22.503879Z","shell.execute_reply":"2025-07-17T14:06:22.512909Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1476 entries, 0 to 1475\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      1476 non-null   int64 \n 1   text    1476 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 23.2+ KB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:25.175467Z","iopub.execute_input":"2025-07-17T14:06:25.175794Z","iopub.status.idle":"2025-07-17T14:06:25.183783Z","shell.execute_reply.started":"2025-07-17T14:06:25.175747Z","shell.execute_reply":"2025-07-17T14:06:25.183061Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label\nnot_applicable    3697\nhope              1892\nhate              1301\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Discover Our Dataset","metadata":{}},{"cell_type":"code","source":"import emoji\n#Stats about Text\ndef avg_word(sentence):\n    words = sentence.split()\n    if len(words) == 0:\n        return 0\n    return (sum(len(word) for word in words)/len(words))\n\ndef emoji_counter(sentence):\n    return emoji.emoji_count(sentence)\n\ntrain['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\ntrain['char_count'] = train['text'].str.len() ## this also includes spaces\ntrain['avg_char_per_word'] = train['text'].apply(lambda x: avg_word(x))\nstop = stopwords.words('arabic')\ntrain['stopwords'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntrain['emoji_count'] = train['text'].apply(lambda x: emoji_counter(x))\ntrain = train.sort_values(by='word_count',ascending=[0])\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:28.251435Z","iopub.execute_input":"2025-07-17T14:06:28.251714Z","iopub.status.idle":"2025-07-17T14:06:30.614448Z","shell.execute_reply.started":"2025-07-17T14:06:28.251694Z","shell.execute_reply":"2025-07-17T14:06:30.613595Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        id                                               text           label  \\\n6866  2558  وأعرضتِ حتّى لا أراك وإنّما أَرى منكِ وَجهَ ال...  not_applicable   \n6032  1908  اِسلُك بِعِزِّكَ هذا أَحسَن السُبُل فَاِنَّ عِ...            hope   \n2353  7415  قَالَتْ وقَدْ سَمِعَتْ شِعْرِي فَأَعْجَبَها إِ...  not_applicable   \n6877  6949  رعى اللَه دهراً فات لم أقض حقه وقد كنت طباً با...  not_applicable   \n398   2577  إن الثناء على الأسماء أجمعها بها وليس سواها يع...  not_applicable   \n\n      word_count  char_count  avg_char_per_word  stopwords  emoji_count  \n6866         107         601           4.626168         24            0  \n6032         105         655           5.247619         14            0  \n2353         104         782           6.528846          8            0  \n6877         101         499           3.950495         21            0  \n398          100         505           4.060000         33            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>avg_char_per_word</th>\n      <th>stopwords</th>\n      <th>emoji_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6866</th>\n      <td>2558</td>\n      <td>وأعرضتِ حتّى لا أراك وإنّما أَرى منكِ وَجهَ ال...</td>\n      <td>not_applicable</td>\n      <td>107</td>\n      <td>601</td>\n      <td>4.626168</td>\n      <td>24</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6032</th>\n      <td>1908</td>\n      <td>اِسلُك بِعِزِّكَ هذا أَحسَن السُبُل فَاِنَّ عِ...</td>\n      <td>hope</td>\n      <td>105</td>\n      <td>655</td>\n      <td>5.247619</td>\n      <td>14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2353</th>\n      <td>7415</td>\n      <td>قَالَتْ وقَدْ سَمِعَتْ شِعْرِي فَأَعْجَبَها إِ...</td>\n      <td>not_applicable</td>\n      <td>104</td>\n      <td>782</td>\n      <td>6.528846</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6877</th>\n      <td>6949</td>\n      <td>رعى اللَه دهراً فات لم أقض حقه وقد كنت طباً با...</td>\n      <td>not_applicable</td>\n      <td>101</td>\n      <td>499</td>\n      <td>3.950495</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>2577</td>\n      <td>إن الثناء على الأسماء أجمعها بها وليس سواها يع...</td>\n      <td>not_applicable</td>\n      <td>100</td>\n      <td>505</td>\n      <td>4.060000</td>\n      <td>33</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import re\ndef count_total_emojis(df):\n    # Unicode ranges for emojis\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n                               u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n                               \"]+\", flags=re.UNICODE)\n\n    # Helper function to count emojis in a text\n    def count_emojis(text):\n        return len(emoji_pattern.findall(text))\n\n    # Total emoji count\n    emoji_count = 0\n    \n    for text in df['text']:\n        # Count emojis in the entire text\n        emoji_count += count_emojis(text)\n\n    return emoji_count\n\nprint(count_total_emojis(train),count_total_emojis(dev))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:33.833334Z","iopub.execute_input":"2025-07-17T14:06:33.834007Z","iopub.status.idle":"2025-07-17T14:06:33.919852Z","shell.execute_reply.started":"2025-07-17T14:06:33.833981Z","shell.execute_reply":"2025-07-17T14:06:33.919117Z"}},"outputs":[{"name":"stdout","text":"1426 291\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# TEXT CLEANING","metadata":{}},{"cell_type":"markdown","source":"## Text standarisation","metadata":{}},{"cell_type":"code","source":"!pip install tashaphyne unidecode nltk textblob pyarabic transliterate aiogoogletrans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:07:35.299425Z","iopub.execute_input":"2025-07-17T14:07:35.300288Z","iopub.status.idle":"2025-07-17T14:07:44.040929Z","shell.execute_reply.started":"2025-07-17T14:07:35.300257Z","shell.execute_reply":"2025-07-17T14:07:44.040024Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tashaphyne in /usr/local/lib/python3.11/dist-packages (0.3.6)\nRequirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\nRequirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\nRequirement already satisfied: transliterate in /usr/local/lib/python3.11/dist-packages (1.10.2)\nCollecting aiogoogletrans\n  Downloading aiogoogletrans-3.3.3.tar.gz (16 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\nCollecting httpx==0.23.0 (from httpx[http2]==0.23.0->aiogoogletrans)\n  Downloading httpx-0.23.0-py3-none-any.whl.metadata (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (2025.6.15)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (1.3.1)\nCollecting rfc3986<2,>=1.3 (from rfc3986[idna2008]<2,>=1.3->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore<0.16.0,>=0.15.0 (from httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans)\n  Downloading httpcore-0.15.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]==0.23.0->aiogoogletrans) (4.2.0)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]==0.23.0->aiogoogletrans) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]==0.23.0->aiogoogletrans) (4.1.0)\nCollecting h11<0.13,>=0.11 (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans)\n  Downloading h11-0.12.0-py3-none-any.whl.metadata (8.1 kB)\nCollecting anyio==3.* (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans)\n  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx==0.23.0->httpx[http2]==0.23.0->aiogoogletrans) (3.10)\nDownloading httpx-0.23.0-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading h11-0.12.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: aiogoogletrans\n  Building wheel for aiogoogletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for aiogoogletrans: filename=aiogoogletrans-3.3.3-py3-none-any.whl size=15693 sha256=0a84ac7d1186bf92ac84412d50b0d74e368cdb6fa8f68b0a37245d2020ad0d2e\n  Stored in directory: /root/.cache/pip/wheels/e7/cd/bf/16932a25e71e20e71b526badb5be32c859dc073ca99cd5cea8\nSuccessfully built aiogoogletrans\nInstalling collected packages: rfc3986, h11, anyio, httpcore, httpx, aiogoogletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.16.0\n    Uninstalling h11-0.16.0:\n      Successfully uninstalled h11-0.16.0\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.9.0\n    Uninstalling anyio-4.9.0:\n      Successfully uninstalled anyio-4.9.0\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.9\n    Uninstalling httpcore-1.0.9:\n      Successfully uninstalled httpcore-1.0.9\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-genai 1.21.1 requires anyio<5.0.0,>=4.8.0, but you have anyio 3.7.1 which is incompatible.\ngoogle-genai 1.21.1 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.23.0 which is incompatible.\ngradio-client 1.10.1 requires httpx>=0.24.1, but you have httpx 0.23.0 which is incompatible.\nfirebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.23.0 which is incompatible.\ngradio 5.31.0 requires httpx>=0.24.1, but you have httpx 0.23.0 which is incompatible.\nlangchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiogoogletrans-3.3.3 anyio-3.7.1 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 rfc3986-1.5.0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport re\n# from dsaraby import DSAraby\n# ds = DSAraby()\nfrom tashaphyne.stemming import ArabicLightStemmer\nfrom nltk.stem.isri import ISRIStemmer\nfrom transliterate import translit\n\nstops = set(stopwords.words(\"arabic\"))\nstop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\nArListem = ArabicLightStemmer()\n\n# def to_arabic(text):\n#     return ds.transliterate(text)\ndef to_arabic(text):\n    try:\n        return translit(text, 'ar')\n    except:\n        return text\n\ndef stem(text):\n    zen = TextBlob(text)\n    words = zen.words\n    cleaned = list()\n    for w in words:\n        ArListem.light_stem(w)\n        cleaned.append(ArListem.get_root())\n    return \" \".join(cleaned)\n\nimport pyarabic.araby as araby\ndef normalizeArabic(text):\n    text = text.strip()\n    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    noise = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(noise, '', text)\n    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n    return araby.strip_tashkeel(text)\n    \ndef remove_stop_words(text):\n    zen = TextBlob(text)\n    words = zen.words\n    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:47.968111Z","iopub.execute_input":"2025-07-17T14:06:47.968456Z","iopub.status.idle":"2025-07-17T14:06:48.143118Z","shell.execute_reply.started":"2025-07-17T14:06:47.968425Z","shell.execute_reply":"2025-07-17T14:06:48.142305Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Deal with hashtags","metadata":{}},{"cell_type":"code","source":"def split_hashtag_to_words(tag):\n    tag = tag.replace('#','')\n    tags = tag.split('_')\n    if len(tags) > 1 :\n        return tags\n    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n    return pattern.findall(tag)\n\ndef clean_hashtag(text):\n    words = text.split()\n    text = list()\n    for word in words:\n        if is_hashtag(word):\n            text.extend(extract_hashtag(word))\n        else:\n            text.append(word)\n    return \" \".join(text)\ndef is_hashtag(word):\n    if word.startswith(\"#\"):\n        return True\n    else:\n        return False\ndef extract_hashtag(text):\n    \n    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n    word_list = []\n    for word in hash_list :\n        word_list.extend(split_hashtag_to_words(word))\n    return word_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:50.935438Z","iopub.execute_input":"2025-07-17T14:06:50.935740Z","iopub.status.idle":"2025-07-17T14:06:50.942879Z","shell.execute_reply.started":"2025-07-17T14:06:50.935717Z","shell.execute_reply":"2025-07-17T14:06:50.941987Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Dealing with emojis in a string","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/arabic-emojis/emojis.csv','r',encoding='utf-8') as f:\n    lines = f.readlines()\n    emojis_ar = {}\n    for line in lines:\n        line = line.strip('\\n').split(';')\n        emojis_ar.update({line[0].strip():line[1].strip()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:06:54.313394Z","iopub.execute_input":"2025-07-17T14:06:54.313677Z","iopub.status.idle":"2025-07-17T14:06:54.334782Z","shell.execute_reply.started":"2025-07-17T14:06:54.313659Z","shell.execute_reply":"2025-07-17T14:06:54.334102Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from __future__ import unicode_literals\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n\nimport unicodedata\nfrom unidecode import unidecode\n\ndef emoji_native_translation(text):\n    text = text.lower()\n    loves = [\"<3\", \"♥\",'❤']\n    smilefaces = []\n    sadfaces = []\n    neutralfaces = []\n\n    eyes = [\"8\",\":\",\"=\",\";\"]\n    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n    for e in eyes:\n        for n in nose:\n            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n                smilefaces.append(e+n+s)\n                smilefaces.append(e+s)\n            for s in [\"\\(\", \"\\[\", \"{\"]:\n                sadfaces.append(e+n+s)\n                sadfaces.append(e+s)\n            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n                neutralfaces.append(e+n+s)\n                neutralfaces.append(e+s)\n            #reversed\n            for s in [\"\\(\", \"\\[\", \"{\"]:\n                smilefaces.append(s+n+e)\n                smilefaces.append(s+e)\n            for s in [\"\\)\", \"\\]\", \"}\"]:\n                sadfaces.append(s+n+e)\n                sadfaces.append(s+e)\n            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n                neutralfaces.append(s+n+e)\n                neutralfaces.append(s+e)\n\n    smilefaces = list(set(smilefaces))\n    sadfaces = list(set(sadfaces))\n    neutralfaces = list(set(neutralfaces))\n    t = []\n    for w in text.split():\n        if w in loves:\n            t.append(\"حب\")\n        elif w in smilefaces:\n            t.append(\"مضحك\")\n        elif w in neutralfaces:\n            t.append(\"عادي\")\n        elif w in sadfaces:\n            t.append(\"محزن\")\n        else:\n            t.append(w)\n    newText = \" \".join(t)\n    return newText\n\nimport emoji\ndef is_emoji(word):\n    if word in emojis_ar:\n        return True\n    else:\n         return False\n    \ndef add_space(text):\n    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n\nfrom aiogoogletrans import Translator\ntranslator = Translator()\nimport asyncio\nloop = asyncio.get_event_loop()\ndef translate_emojis(words):\n    word_list = list()\n    words_to_translate = list()\n    for word in words :\n        t = emojis_ar.get(word.get('emoji'),None)\n        if t is None:\n            word.update({'translation':'عادي','translated':True})\n            #words_to_translate.append('normal')\n        else:\n            word.update({'translated':False,'translation':t})\n            words_to_translate.append(t.replace(':','').replace('_',' '))\n        word_list.append(word)\n    return word_list\n\ndef emoji_unicode_translation(text):\n    text = add_space(text)\n    words = text.split()\n    text_list = list()\n    emojis_list = list()\n    c = 0\n    for word in words:\n        if is_emoji(word):\n            emojis_list.append({'emoji':word,'emplacement':c})\n        else:\n            text_list.append(word)\n        c+=1\n    emojis_translated = translate_emojis(emojis_list)\n    for em in emojis_translated:\n        text_list.insert(em.get('emplacement'),em.get('translation'))\n    text = \" \".join(text_list)\n    return text\n    \ndef clean_emoji(text):\n    text = emoji_native_translation(text)\n    text = emoji_unicode_translation(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:07:47.174047Z","iopub.execute_input":"2025-07-17T14:07:47.175009Z","iopub.status.idle":"2025-07-17T14:07:47.679552Z","shell.execute_reply.started":"2025-07-17T14:07:47.174971Z","shell.execute_reply":"2025-07-17T14:07:47.678489Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def clean_tweet(text):\n    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n    text = re.sub('@[^\\s]+',' ',text)\n    text = clean_hashtag(text)\n    text = clean_emoji(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:08:25.612578Z","iopub.execute_input":"2025-07-17T14:08:25.612907Z","iopub.status.idle":"2025-07-17T14:08:25.617573Z","shell.execute_reply.started":"2025-07-17T14:08:25.612883Z","shell.execute_reply":"2025-07-17T14:08:25.616824Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def clean_text(text):\n    ## Clean for tweets\n    text = clean_tweet(text)\n    ## Remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n    ## remove extra whitespace\n    text = re.sub('\\s+', ' ', text)  \n    ## Remove Emojis\n    text = remove_emoji(text)\n    ## Convert text to lowercases\n    text = text.lower()\n    ## Arabisy the text\n    text = to_arabic(text)\n    ## Remove stop words\n    text = remove_stop_words(text)\n    ## Remove numbers\n    text = re.sub(\"\\d+\", \" \", text)\n    ## Remove Tashkeel\n    text = normalizeArabic(text)\n    #text = re.sub('\\W+', ' ', text)\n    text = re.sub('[A-Za-z]+',' ',text)\n    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n    ## remove extra whitespace\n    text = re.sub('\\s+', ' ', text)  \n    #Stemming\n    #text = stem(text)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x:clean_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:08:27.899217Z","iopub.execute_input":"2025-07-17T14:08:27.899541Z","iopub.status.idle":"2025-07-17T14:08:32.432363Z","shell.execute_reply.started":"2025-07-17T14:08:27.899502Z","shell.execute_reply":"2025-07-17T14:08:32.431658Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"### Common word removal\nfreq = pd.Series(' '.join(train['text']).split()).value_counts()[:12]\nfreq = list(freq.index)\ntrain['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n### Rare words removal\nfreq = pd.Series(' '.join(train['text']).split()).value_counts()[-50:]\nfreq = list(freq.index)\ntrain['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n### Remove Numbers from text\ntrain['text'] = train['text'].str.replace(r'\\d+', '', regex=True).str.strip().str.replace(r'\\s+', ' ', regex=True)\n\n### Discouver Data again after cleaning\ntrain['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\ntrain['char_count'] = train['text'].str.len() ## this also includes spaces\ntrain['avg_char_per_word'] = train['text'].apply(lambda x: avg_word(x))\nstop = stopwords.words('arabic')\ntrain['stopwords'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntrain['emoji_count'] = train['text'].apply(lambda x: emoji_counter(x))\ntrain = train.sort_values(by='word_count',ascending=[0])\nfinal = []\nfor index, row in train.iterrows():\n    if len(row['text'].split()) > 3:\n        final.append([row['text'],row['label']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:10:02.099447Z","iopub.execute_input":"2025-07-17T14:10:02.099752Z","iopub.status.idle":"2025-07-17T14:10:04.706513Z","shell.execute_reply.started":"2025-07-17T14:10:02.099731Z","shell.execute_reply":"2025-07-17T14:10:04.705641Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df = pd.DataFrame(final)\ndf.columns = ['text','label']\ndf.to_csv('final_nice.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:10:09.796550Z","iopub.execute_input":"2025-07-17T14:10:09.796890Z","iopub.status.idle":"2025-07-17T14:10:09.843127Z","shell.execute_reply.started":"2025-07-17T14:10:09.796865Z","shell.execute_reply":"2025-07-17T14:10:09.842345Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:10:12.939143Z","iopub.execute_input":"2025-07-17T14:10:12.939440Z","iopub.status.idle":"2025-07-17T14:10:12.948210Z","shell.execute_reply.started":"2025-07-17T14:10:12.939418Z","shell.execute_reply":"2025-07-17T14:10:12.947297Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                text label\n0  مري الدمع عينيك دار محيله بفيض الحشا تسفي عليه...  hope\n1  اديرا الكاس ينقشع الغم ولا تحبسا كاسي ففي حبسه...  hope\n2  ايا ماجدا مذ يمم المجد نكص وبدر تمام مذ تكامل ...  hope\n3  اسلك بعزك احسن السبل فان عزك بالصديق الازل واف...  hope\n4  غزال به فتر وفيه تانث واحسن مخلوق واجمل مشي اق...  hope","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>مري الدمع عينيك دار محيله بفيض الحشا تسفي عليه...</td>\n      <td>hope</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>اديرا الكاس ينقشع الغم ولا تحبسا كاسي ففي حبسه...</td>\n      <td>hope</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ايا ماجدا مذ يمم المجد نكص وبدر تمام مذ تكامل ...</td>\n      <td>hope</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اسلك بعزك احسن السبل فان عزك بالصديق الازل واف...</td>\n      <td>hope</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>غزال به فتر وفيه تانث واحسن مخلوق واجمل مشي اق...</td>\n      <td>hope</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}